{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df719788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shoai\\anaconda3\\envs\\aivenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shoai\\.cache\\huggingface\\hub\\models--gpt2-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "=== GENERATED STORY ===\n",
      "\n",
      "In a distant future, humanity has discovered the existence of an alternate world in which all life was genetically engineered by AI. On this new universe people are able to create their own organic beings under these artificial minds and they can then reproduce as many copies that is physically possible without genetic engineering—even if only slightly more intelligent than one another [12], using cloning technology instead). The original population began disappearing from Earth but it isn't known what caused them or where we should go after there have been no sightings for years (and maybe ever before).[13]\n",
      "Fictionalization One major issue with Star Trek: Deep Space Nine's depiction could be its use within fiction; such characters exist virtually nowhere else on television.[14][15]. This seems especially troubling when considering how much time exists between episodes across multiple networks despite being written at different times! For example[16]: In season seven episode eight \"The Emissary\", Dr McCoy states his belief-based philosophy during Spock Prime speech regarding science vs religion - 'if you believe something like I do, perhaps someone shouldn´t get mad about me because sometimes your faith may lead inevitably to misunderstanding.' But not until Kirk asks him later would he accept scientific method back into Klingons' society – although given our understanding now today so early access towards alien ethics might allow us see things differently going forward? It remains uncertain whether DS9 will provide sufficient guidance against any misuse arising outwards rather Than inwardly…\n",
      "\n",
      " Plus though several attempts were made following TOS & ENT novels , including various versions involving Romulan scientists/philosophers working hand‑in‐hand through diplomacy... none really worked very well except among fans who thought those stories didn\\'T reflect real lives.... So just try writing some stuff up too right off the bat :p We cannot predict further development beyond Season 7 Episode 8 (\"Time Beyond\") unless George Lucas decides otherwise again...... *edit*: An interesting followup suggestion comes via @Darth_Haxon1 over here (#1169) If Picard does actually find himself talking to somebody other Thalassa types throughout 2045..... He knows why!? His memory implants probably know.. .A recent article appeared suggesting that Chinese students think fewer Westerners speak English better\n",
      "\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# QUESTION 2: TEXT GENERATION WITH TRANSFORMERS\n",
    "# ============================================\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# ============================================\n",
    "# 1. LOAD PRETRAINED GPT MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading GPT-2 model...\")\n",
    "\n",
    "model_name = \"gpt2-medium\"  # You can also use \"gpt2-medium\" for better quality\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "# ============================================\n",
    "# 2. TEXT GENERATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def generate_text(prompt,\n",
    "                  max_length=450,\n",
    "                  min_length=200,\n",
    "                  temperature=0.9,\n",
    "                  top_k=50,\n",
    "                  top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generates a long, high-quality story using GPT-2.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,       # long story\n",
    "        min_length=min_length,       # prevents early stopping\n",
    "        temperature=temperature,     # controls creativity\n",
    "        top_k=top_k,                 # top-k sampling\n",
    "        top_p=top_p,                 # nucleus sampling\n",
    "        do_sample=True,              # enable creative sampling\n",
    "        repetition_penalty=1.2,      # reduce repeating loops\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=None            # do NOT stop early\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ============================================\n",
    "# 3. EXAMPLE PROMPT\n",
    "# ============================================\n",
    "\n",
    "prompt = \"In a distant future, humanity has discovered\"\n",
    "generated_story = generate_text(prompt)\n",
    "\n",
    "# ============================================\n",
    "# 4. OUTPUT\n",
    "# ============================================\n",
    "\n",
    "print(\"=== GENERATED STORY ===\\n\")\n",
    "print(generated_story)\n",
    "print(\"\\n========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d77da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
